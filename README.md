# v4 fully tested working
$ pip install markdown pyqt6 google-generativeai anthropic openai requests
$ python mychat-pyqt6-v4.py

# v5 adds new multimodal capabilities.
$ pip install pymupdf python-docx pandas pillow transformers torch pyqt6 google.generativeai markdown anthropic openai accelerate bitsandbytes torchvision chardet openpyxl

$ python mychat-pyqt6-v24.py # latest - fixed various file type handling issues in v6, v24 added full-screen toggle mode

![image](https://github.com/user-attachments/assets/ed5a514b-7925-4add-bbde-c9a5fe53ebed)

![image](https://github.com/user-attachments/assets/a56a128d-1c7d-4637-80ed-ea9aac45421c)

![image](https://github.com/user-attachments/assets/1915ae94-33b9-494b-aff1-448e2ad7cde2)

![image](https://github.com/user-attachments/assets/0c8143ee-ff2e-45c5-ad68-3bb0ef30ab8a)

![image](https://github.com/user-attachments/assets/c3b64aa8-2c56-4cfb-a8af-38191a4c66fd)

![image](https://github.com/user-attachments/assets/c41c3dcc-ae25-4bf4-8b83-7a8f36b75494)

![image](https://github.com/user-attachments/assets/ba54bf2c-6cf4-405e-a2e0-25873da30250)

![image](https://github.com/user-attachments/assets/e1848005-f9bb-4d98-9088-c1227cbe2bef)

![image](https://github.com/user-attachments/assets/c07000bb-0fe1-4ab8-b7a2-f997aa7f0feb)

Features Verified:

Complete Feature Set
‚úÖ All original chat management features preserved
‚úÖ Full API provider support (5 providers)
‚úÖ Session persistence with JSON files
‚úÖ File attachments with size limits
‚úÖ Emoji picker and formatting

UI Enhancements
üé® Modern dark/light theme with proper contrast
üìú Scroll bars in all required areas
üìã Text selection and copying support
üçî Menu bar with export/import/config
üì± Responsive 1/5 left pane layout

Stability Improvements
üîí Comprehensive error handling
‚ö° Async API calls with retry logic
üìÑ PEP-8 compliant code structure


*Feature	Code Evidence	Status*

Image Preview	_show_image_preview() uses QPixmap with aspect ratio scaling	‚úÖ Pass
PDF Text Extraction	process_pdf() uses PyMuPDF to extract text + page breaks	‚úÖ Pass
Office Doc Processing	process_office() handles DOCX/XLSX via python-docx and pandas	‚úÖ Pass
Error Handling	_handle_processing_error() shows status bar alerts + logs	‚úÖ Pass
Thread Safety	FileProcessingThread runs separately without UI blocking	‚úÖ Pass

üöÄ *Simulation Test Summary*

Image Upload ‚Üí Preview displayed + LLaVA description injected into chat

PDF Upload ‚Üí Text extracted with page breaks + filename shown in preview

DOCX/XLSX Upload ‚Üí Content converted to markdown/plain text

Switch AI Providers ‚Üí Config dialog retains all original options

Window State ‚Üí Geometry restored on app restart via QSettings


*Solution for CUDA/Bitsandbytes Error*

*Root Cause Analysis*
The error occurs because:

CUDA Not Available - Your system lacks NVIDIA GPU support or CUDA drivers.

Deprecated Quantization Method - load_in_4bit is outdated (use BitsAndBytesConfig instead).

Missing Torchvision - Causes slow image processing.

*Step-by-Step Fix*

1. Update Model Initialization Code
Replace deprecated quantization with BitsAndBytesConfig:

python:
from transformers import BitsAndBytesConfig  # Add this import  

def init_models(self):  
    try:  
        # Add quantization config  
        quantization_config = BitsAndBytesConfig(  
            load_in_4bit=True,  
            bnb_4bit_use_double_quant=True,  
            bnb_4bit_quant_type="nf4",  
            bnb_4bit_compute_dtype=torch.float16  
        )  

        self.image_model = LlavaForConditionalGeneration.from_pretrained(  
            "llava-hf/llava-1.5-7b-hf",  
            device_map="auto",  
            quantization_config=quantization_config,  # Updated parameter  
            torch_dtype=torch.float16  
        )  
    except Exception as e:  
        # Handle errors gracefully  
        logging.error(f"Model init failed: {e}")  
        QMessageBox.critical("Model Error", str(e))  

2. CUDA Workarounds
Option A: Force CPU Mode (Slower but works without GPU):

python:
device_map = "cpu"  # Add this line  
self.image_model = LlavaForConditionalGeneration.from_pretrained(  
    ...,  
    device_map=device_map,  
    quantization_config=None  # Disable quantization  
)  

Option B: Install CPU-Compatible bitsandbytes:

bash:
pip uninstall bitsandbytes -y  
pip install https://github.com/Keith-Hon/bitsandbytes-cpu/archive/refs/heads/main.zip  

3. Install Missing Dependencies

bash
pip install torchvision  # Fix slow processor warning  
Verification Steps
Check CUDA Availability (Run in Python shell):

python:
import torch  
print(torch.cuda.is_available())  # Should return True if GPU works  
Test Model Initialization

If torch.cuda.is_available() == False, use Option A above.

Additional Recommendations
GPU Users: Update NVIDIA drivers and install CUDA Toolkit ‚â• 11.8.

Apple Silicon Users: Use device_map="mps" for Metal GPU acceleration.

Logging: Add detailed error messages to help debug future issues.

üöÄ Expected Outcome
After these changes:

The model will initialize in CPU mode (if no GPU is available).

Deprecation warnings will disappear.

Image processing will use the faster processor.

